{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a393b744",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d21b1",
   "metadata": {},
   "source": [
    "- Use DecisionTree class and bootstrapdataset function to build a random forest class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e91fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cd4c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "data = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b22bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Parts of the Code are based on: https://medium.com/@cjakuc/building-a-decision-tree-classifier-c00a08815c3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b23ab",
   "metadata": {},
   "source": [
    "### Implement Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd1ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeManager:\n",
    "    \"\"\"\n",
    "    NodeManager class is used to build the decision tree by storing\n",
    "    the following parameters:\n",
    "    class_prediction: prediction label\n",
    "    feature_idx: idx of feature for optimal split\n",
    "    threshold: threshold for optimal split\n",
    "    left: node below threshold\n",
    "    right: node above threshold\n",
    "    leftbranch/rightbranch: indicate which branch node is from parent\n",
    "    \"\"\"\n",
    "    def __init__(self, class_prediction, depth=None):\n",
    "        self.class_prediction = class_prediction\n",
    "        # feature index will be used to identify optimal split column\n",
    "        self.feature_idx = 0\n",
    "        # threshold will be used to store optimal split threshold\n",
    "        self.threshold = 0\n",
    "        # below threshold\n",
    "        self.left = None\n",
    "        # above threshold\n",
    "        self.right = None\n",
    "        # left to parent\n",
    "        self.leftbranch = False\n",
    "        # right to parent\n",
    "        self.rightbranch = False\n",
    "        self.depth = depth\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    Class to build a decision tree with maximum depth maximum_depth.\n",
    "    Can be used with datasets with a variable number of features.\n",
    "    \"\"\"\n",
    "    def __init__(self, maximum_depth=None):\n",
    "        self.maximum_depth = maximum_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit the decision tree to a training set.\n",
    "        \"\"\"\n",
    "        # identify number of classes\n",
    "        self.n_classes = len(np.unique(y_train))\n",
    "        # build decision tree classifier with build_tree method\n",
    "        self.tree = self.build_tree(X_train, y_train)\n",
    "        #print('...decision tree fitted.')\n",
    "\n",
    "    def next_split(self, data, labels):\n",
    "        \"\"\"\n",
    "        Takes data and labels of a node as input and returns column\n",
    "        and threshold for optimal next split into child nodes with min\n",
    "        impurity. If data contains no samples or minimal\n",
    "        parent impurity was smaller than new minimal \n",
    "        impurity, it returns None,None.\n",
    "        \"\"\"\n",
    "        # Check if the subset contains at least one observation\n",
    "        n_data_points = labels.size\n",
    "        # If not, return None,None for col,threshold\n",
    "        if n_data_points <= 1:\n",
    "            return None, None\n",
    "        \n",
    "        # reshape labels\n",
    "        labels = labels.reshape(n_data_points,)\n",
    "        # count classes in parent node\n",
    "        count_in_parent = [np.count_nonzero(labels == c) for c in range(self.n_classes)]\n",
    "\n",
    "        # Get the minimum impurity of the parent node to compare with new min impurity \n",
    "        parent_impurity = 1.0 - sum((n / n_data_points) ** 2 for n in count_in_parent)\n",
    "        \n",
    "        # combine data and labels to facilitate sorting\n",
    "        dataset = np.column_stack((data,labels))\n",
    "        # Initialize empty G_list\n",
    "        G_list = []\n",
    "\n",
    "        # iteraete over columns (features)\n",
    "        for col in range(0, dataset.shape[1]-1):\n",
    "            # sort by respective column\n",
    "            dataset = dataset[dataset[:, col].argsort()]\n",
    "            \n",
    "            # empty list to store results for every column\n",
    "            G_col_list = []\n",
    "\n",
    "            # iterate over elements in col\n",
    "            for i in range(0,len(dataset)-1):\n",
    "                # compute threshold as mean of two elements\n",
    "                threshold = (dataset[i,col] + dataset[i+1,col])/2\n",
    "                # number of values equal above threshold\n",
    "                n1 = (dataset[:,col] >= threshold).sum()\n",
    "                # number of values below threshold\n",
    "                n2 = (dataset[:,col] < threshold).sum()\n",
    "                # total number of elements\n",
    "                n_all = n1+n2\n",
    "                # number of elements in node 1 which belong to class 0\n",
    "                l1_class0 = (dataset[dataset[:,col]>=threshold][:,-1] == 0).sum()\n",
    "                # number of elements in node 1 which belong to class 1\n",
    "                l1_class1 = (dataset[dataset[:,col]>=threshold][:,-1] == 1).sum()\n",
    "                # gini impurity of node 1\n",
    "                if n1 != 0:\n",
    "                    l1_G = 1 - (l1_class0/n1)**2 - (l1_class1/n1)**2\n",
    "                else: \n",
    "                    l1_G = 0\n",
    "                # number of elements in node 2 which belong to class 0\n",
    "                l2_class0 = (dataset[dataset[:,col]<threshold][:,-1] == 0).sum()\n",
    "                # number of elements in node 2 which belong to class 1\n",
    "                l2_class1 = (dataset[dataset[:,col]<threshold][:,-1] == 1).sum()\n",
    "                # gini impurity of node 2\n",
    "                if n2 != 0:\n",
    "                    l2_G = 1 - (l2_class0/n2)**2 - (l2_class1/n2)**2\n",
    "                else:\n",
    "                    l2_G = 0\n",
    "                # gini impurity of whole node (weightes impurities of node 1 and node 2)\n",
    "                G = n1/n_all*l1_G + n2/n_all*l2_G\n",
    "                # store gini impority and threshold\n",
    "                result = np.array([threshold, G])\n",
    "                G_col_list.append(result)\n",
    "\n",
    "            # identify and store smallest gini impurity\n",
    "            G_col_arr = np.array(G_col_list)\n",
    "            idx_smallest_G = np.array(G_col_list)[:,1].argmin()\n",
    "            G_list.append(G_col_arr[idx_smallest_G])\n",
    "\n",
    "        G_arr = np.array(G_list)\n",
    "        # The column with the smallest impurity\n",
    "        optimal_col = G_arr[:,-1].argmin()\n",
    "        # min gini impurity\n",
    "        min_impurity = G_arr[optimal_col, -1]\n",
    "        # The respective threshold of the column with the smallest impurity\n",
    "        optimal_threshold = G_arr[optimal_col][0]\n",
    "        \n",
    "        # check if min_impurity is larger than parent impurity (no improvement):\n",
    "        # If this is the case, return None, None for col and threshold\n",
    "        if min_impurity >= parent_impurity:\n",
    "            return None,None\n",
    "\n",
    "        # If impurity improved, return the optimal column and the optimal threshold\n",
    "        return optimal_col, optimal_threshold\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        \"\"\"\n",
    "        Build the decision tree until the maximum depth is reached.\n",
    "        \"\"\"\n",
    "\n",
    "        # count the occurance of each class\n",
    "        class_occurrence = [np.count_nonzero(y == i) for i in range(self.n_classes)]\n",
    "        # class prediction is the class with the largest occurence\n",
    "        class_prediction = np.argmax(class_occurrence)\n",
    "        # Instantiate an empty Node using the NodeManager class\n",
    "        node = NodeManager(class_prediction=class_prediction,depth=depth)\n",
    "        # Create a variable in the node class for the number of samples\n",
    "        node.samples = y.size\n",
    "        # As long as max depth is not reached...\n",
    "        if depth < self.maximum_depth:\n",
    "            # apply next_split method to get col and threshold of optimal split\n",
    "            # Returns None of Leaf is reached (branch is finished)\n",
    "            col, threshold = self.next_split(X, y)\n",
    "            # If leaf is not reached...\n",
    "            if col and threshold:\n",
    "                # split the data and labels into right and left using col and threshold\n",
    "                # left index for data points below threshold\n",
    "                left_idx = X[:, col] < threshold\n",
    "                # right index for datapoints eqal or above threshold\n",
    "                right_idx = X[:, col] >= threshold\n",
    "                # dataset of left node\n",
    "                X_left = X[left_idx]\n",
    "                y_left = y[left_idx]\n",
    "                # dataset of right node\n",
    "                X_right = X[right_idx]\n",
    "                y_right = y[right_idx]\n",
    "                # Store parameters to Node manager\n",
    "                node.feature_idx = col\n",
    "                node.threshold = threshold\n",
    "                # Apply build_tree function inside build_tree to build the next level of nodes with left node\n",
    "                # Increase depth counter by 1\n",
    "                node.left = self.build_tree(X_left, y_left, depth+1)\n",
    "                # store info that node was left side of parent node in node manager\n",
    "                node.left.leftbranch = True\n",
    "                # Apply build_tree function inside build_tree to build the next level of nodes with right node\n",
    "                node.right = self.build_tree(X_right, y_right, depth+1)\n",
    "                # store info that node was right side of parent node in node manager\n",
    "                node.right.rightbranch = True\n",
    "                \n",
    "        return node\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Takes X_test as input and predicts labels\n",
    "        using the built decision tree. \n",
    "        Uses the stored information about split col,\n",
    "        split threshold, left or right, leftbranch or \n",
    "        right branch from NodeManager.\n",
    "        \"\"\"\n",
    "        # assign root node\n",
    "        node = self.tree\n",
    "        # empty prediction list\n",
    "        y_preds = []\n",
    "        # Iterate over data points in X_test\n",
    "        for data_point in X_test:\n",
    "            # Reasign root node\n",
    "            node = self.tree\n",
    "            # Go through the decision nodes of the tree and use optimal\n",
    "            # split columns and optimal thresholds to predict\n",
    "            # classification of data point.\n",
    "            while node.left:\n",
    "                # Make Predictions with left node\n",
    "                if data_point[node.feature_idx] < node.threshold:\n",
    "                    node = node.left\n",
    "                # Make Predictions with right node\n",
    "                else:\n",
    "                    node = node.right\n",
    "            # append prediction to prediction list\n",
    "            y_preds.append(node.class_prediction)\n",
    "            \n",
    "        # return predictions as numpy array\n",
    "        return np.array(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42f13e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapdataset(data, nsamples= -1):\n",
    "    \"\"\"\n",
    "    Returns new dataset of nsamples which are \n",
    "    sampled randomly with replacement.\n",
    "    \"\"\"\n",
    "    # size of bootstrap dataset\n",
    "    if nsamples==-1:\n",
    "        size = len(data)\n",
    "    else:\n",
    "        size=nsamples\n",
    "    # index array of entire train data\n",
    "    data_idx = np.arange(len(data))\n",
    "    \n",
    "    # sample random bootstap from index array including duplicates (replacement)\n",
    "    bootstrapped_idx = np.random.choice(data_idx, size, replace=True)\n",
    "    bootstrapped_dataset = data[bootstrapped_idx]\n",
    "    return bootstrapped_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77a4f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    Class uses bootstrapdataset and DecisionTree\n",
    "    to create a random forest with n trees. \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_trees, max_depth):\n",
    "        \n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.tree_list = []\n",
    "        self.prediction_list = []\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit n decision trees to train data.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tree_list = []\n",
    "        # build trees\n",
    "        for i in range(0,self.n_trees):\n",
    "            # join data and labels\n",
    "            data = np.column_stack((X_train,y_train))\n",
    "            # use bootstrapdataset to create bootstraps\n",
    "            bootstapped_data = bootstrapdataset(data, nsamples= -1)\n",
    "            X_sub = bootstapped_data[:,:-1]\n",
    "            y_sub = bootstapped_data[:,-1]\n",
    "            # instantiate DecisionTree class\n",
    "            tree = DecisionTree(maximum_depth=self.max_depth)\n",
    "            # fit tree with bootstap data\n",
    "            tree.fit(X_sub, y_sub)\n",
    "            # append fitted tree to tree_list\n",
    "            self.tree_list.append(tree)\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predict on test data.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.prediction_list = []\n",
    "        # store prediction of every tree to prediction list\n",
    "        for tree in self.tree_list:\n",
    "            tree.predict(X_test)\n",
    "            y_pred_tree = np.array(tree.predict(X_test))\n",
    "            self.prediction_list.append(y_pred_tree)\n",
    "            \n",
    "        predictions = np.array(self.prediction_list)\n",
    "        # compute the sum of the predictions along the 0-axis\n",
    "        # e.g. a datapoint which was five times assigned to class 1 gets a sum_prediction value of 5\n",
    "        sum_predictions = np.sum(predictions, axis=0)\n",
    "        # majority vote: predict class 1 if at least half of the trees predicted 1, class 0 otherwise\n",
    "        y_pred = np.where(sum_predictions>=(self.n_trees/2), 1, 0)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1c082",
   "metadata": {},
   "source": [
    "### Test Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c549fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, shuffle=True, test_size=0.3):\n",
    "    \"\"\"\n",
    "    Split data into training and testing set.\n",
    "    \"\"\"\n",
    "    n_train = int(X.shape[0]*(1-test_size))\n",
    "    indices = np.arange(len(X))\n",
    "    if shuffle: \n",
    "        np.random.shuffle(indices)\n",
    "    train = indices[:n_train]\n",
    "    test = indices[n_train:]\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a85c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# train test split (30% for testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X=X, y=y, shuffle=True, test_size=0.3)\n",
    "\n",
    "# For binary classification: merge Iris virginica and Iris setosa into one class.\n",
    "y_train = np.where(y_train==2, 0, y_train)\n",
    "y_test = np.where(y_test==2, 0, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "012ba159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_pred, y_test):\n",
    "    \"\"\"\n",
    "    Takes y_test and y_pred as input and\n",
    "    returns accuracy.\n",
    "    \"\"\"\n",
    "    n_accurate = (y_pred == y_test).sum()\n",
    "    n_total = len(y_test)\n",
    "    accuracy = n_accurate / n_total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d546b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy with 1 Trees: 0.8666666666666667\n",
      "Random Forest Accuracy with 3 Trees: 0.9111111111111111\n",
      "Random Forest Accuracy with 5 Trees: 0.8888888888888888\n",
      "Random Forest Accuracy with 10 Trees: 0.9111111111111111\n",
      "Random Forest Accuracy with 20 Trees: 0.9111111111111111\n",
      "Random Forest Accuracy with 50 Trees: 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "# Build Random forest with different amounts of trees and report accuracy\n",
    "sizes = [1, 3, 5, 10, 20, 50]\n",
    "\n",
    "\n",
    "for size in sizes:\n",
    "    random_forest = RandomForest(n_trees=size, max_depth=30)\n",
    "    random_forest.fit(X_train, y_train)\n",
    "    y_pred = random_forest.predict(X_test)\n",
    "    acc = get_accuracy(y_pred, y_test)\n",
    "    print('Random Forest Accuracy with {} Trees: {}'.format(size,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a878ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
